\documentclass[../main.tex]{subfiles}

\begin{document}

\begin{abstract}
Supervised learning in attribute-based spaces is one of the most popular machine learning problems studied and,
consequently, has attracted considerable attention of the genetic algorithm community. The fullmemory approach
developed here uses the same high-level descriptive language that is used in rule-based systems. This allows for an
easy utilization of inference rules of the well-known inductive learning methodology, which replace the traditional
domain-independent operators and make the search task-specific. Moreover, a closer relationship between the underlying
task and the processing mechanisms provides a setting for an application of more powerful task-specific heuristics.
Initial results obtained with a prototype implementation for the simplest case of single concepts indicate that genetic
algorithms can be effectively used to process high-level concepts and incorporate task-specific knowledge. The method
of abstracting the genetic algorithm to the problem level, described here for the supervised inductive learning, can be
also extended to other domains and tasks, since it provides a framework for combining recently popular genetic
algorithm methods with traditional problem-solving methodologies. Moreover, in this particular case, it provides a very
powerful tool enabling study of the widely accepted but not so well understood inductive learning methodology.
\end{abstract}

\begin{mdframed}
\fullcite{Janikow}
\end{mdframed}

\section{Introduction}

With a growing amount of information, knowledge-extraction capabilities have become very important in order to
understand the world. Machine learning is an attempt to solve this problem. There are two main groups of approaches:

\begin{itemize}
	\item \textbf{Non-Symbolic:} Non-symbolic, inductive learning systems, also called sub-symbolic, do not acquire
	explicit knowledge but gather other information necessary for the descriptive model. Examples are
	\emph{statistical models}. All non-symbolic approaches process low-level entities (usually numerical parameters).
	\item \textbf{Symbolic:}  Symbolic systems produce knowledge in a high-level descriptive language. Mechanisms based
	on symbol manipulation, in addition to being closely related to their objectives, allow for the use of the same
	input and output language.
\end{itemize}

Genetic algorithms have been successfully applied to a number of problems. They can and have been used as an approach to
supervised learning. This work follows a different approach than earlier work: rather than trying to extend the set of
operators, we start with a set of inference rules specific to the task, and incorporate them into the genetic algorithm
framework. This leads to a  task-specific problem-solving methodology and abstracts the genetic algorithm's inference to
the problem-specific symbol level. This is a knowledge-intensive approach, because it uses a vast amount of task-
specific information in the genetic operations.

\section{Inductive Learning From Examples}
Concept learning is a fundamental cognitive process that involves learning descriptions
of some categories of objects.  Learning is said to be attribute-based when objects are described by features
(attribute-value pairs). Learning is supervised when examples are pre-classified as belonging to a space.
\\\\
When learning concepts, a mechanism is needed to account for possible cases of no-match and multiple-match when
recognizing new events. The initial design assumes that the subspace not covered by the description is the complement
of the concept.
\\\\
The definition of the input- and output language is an important issue. The input language is an interface between the
environment and the system, and therefore should combine the requirements of both. The output language on the other
hand has to combine the requirements of the system and the application environment.
\\\\
The language $VL_1$ is used. Variables (attributes) are the basic units, having multivalued domains. According to the
relationship among different domain values, such domains may be of different types: nominal, linear or structured with
partially ordered values. relations associate variables with their values by means of selectors with natural semantics.
\\\\
There are two approaches to learning from examples: one approach assumes a working memory that can remember all
previously seen examples (\textbf{Full Memory Learning}) while the other assumes that memory is only available for
generated knowledge. Incremental learning is associated with previously generated knowledge and previously seen
examples. It is an important characteristic because it mimics human learning and because learning systems frequently
display dynamic properties where evidence becomes available from time to time.


\section{Previous Approaches}

\subsection{Traditional Approaches}
Statistical approaches account for the vast majority of non-symbolic, or numerical, approaches. The disadvantage of
these approaches is that they rely on low-level processing for high-level learning. Examples are \textbf{Bayesian} and
\textbf{Neural Network} methods. The fact that this approach performs numerical computations leads to an
apparent advantage of better applicability to processing noisy information.
\\\\
The two prominent symbolic approaches to supervised feature-based learning, recognized
as benchmarks, both of which are full-memory systems.

\begin{itemize}
	\item \textbf{Michalski's AQ} \\
	AQ is based on inductive generalization and specialization of the $VL_1$ formulas using the idea of a cover of the
	positive against the negative events. This approach conceptually follows the ideas of inductive methodology, since
	the generated knowledge is either generalized or specialized.
	\item \textbf{Quinlan's  ID} \\
	With ID, training examples are represented by feature vectors similar to events in $VL_1$.  The algorithm
	constructs a decision tree, where each leaf is associated with a single decision class and each internal node
	corresponds to an attribute while each node's branches correspond to a value of that attribute.
\end{itemize}

\subsection{GA Approaches}
Because a rule-based concept representation means that the number of such rules (disjuncts) is not known a priori,  the
traditional fixed-length representation is unsuitable. Two different approaches have been proposed:

\begin{itemize}
	\item \textbf{Michigan Approach} \\
	The population still consists of fixed-length elements, but the solution is represented by a set of chromosomes
	from the population.
	\item \textbf{Pittsburgh Approach} \\
	Here, variable-length chromosomes are used to represent proposed solutions individually. For supervised learning,
	this would mean each chromosome represents an independent solution.
\end{itemize}

With very few exceptions all systems for rule-based learning use a three-symbol alphabet $\{0, 1, \#\}$. it is not so
well suited for non-binary domains because it increases the representation length. The approach used in this work is to
model closely the non-redundant and feasible only representation of $VL_1$.

\section{The Modified GA}

\subsection{Ideas}
The need for problem-specific knowledge incorporation into GAs was recognized as a method for improvement in many
different domains. Following the Pittsburgh approach, the spectrum of GA designs ranges from classical operations to a
knowledge-intensive approach where operators follow a specific problem-solving methodology. The problem with the
latter is the lack of a theoretical foundation and that it is more distant from nature.
\\\\
All operations in this work are special cases of the traditional operators. This provides an intuitive support for the
same theoretical foundations. Also, because the operators are defined on the semantic pieces of the problem, one may
easily argue that this design naturally satisfies the building-block hypothesis as well.
\\\\
In conclusion, while applications of the traditional domain-independent operators provide for a domain-independent
search conducted in the artificial representation space, we set the genetic algorithm to operate directly in the
problem space by organizing the work there.

\subsection{Representation and Search Space}
The multiple-valued logic language $VL_1$ is chosen for chromosome's representation. The search space is the space of
sets of rules, spanned by given features. It is possible that these descriptions contain redundant parts. Therefore,
only formulas built using the sufficient $=$ relation with internal disjunctions are used.
\\\\
Because of the assumption of learning only a single concept description, all rules are associated with the same single
decision.

\subsection{Initial Population}
The population must initially be filled with possible solutions, either randomly generated or incorporating task
specific knowledge. This problem to generate an initialization that produces actual solutions is as hard as the one
we're trying to solve. A compromise between the two uses three different types:

\begin{enumerate}
	\item \textbf{Random:} Each individual is a set of a random number of complexes, randomly generated on the search
	space.
	\item \textbf{Data:} Each individual is a random positive training event.
	\item \textbf{Hypotheses:} Each individual is just a single hypothesis given a priori.
\end{enumerate}

Actual experiments show that the best average behavior is obtained while using a combination of these three.

\subsection{Evaluation Mechanism}
The evaluation function must reflect the learning criteria. In supervised learning from examples, the criteria include
completeness and consistency. Both are measured with respect to the set of training events.These two measures are
meaningful only to rule sets and individual rules. For conditions, the measures of the parent rule are used.

\begin{equation}
\text{correctness} = \frac{w_1 \cdot \text{completeness} + w_2 \cdot \text{correctness}}{w_1 + w_2}
\end{equation}
\begin{equation}
	\text{evaluation} = \text{correctness} \cdot (1 + w_2 \cdot (1-cost))^f
\end{equation}
Here the cost is normalized on $[0,1]$ and $f$ grows very slowly on $[0,1]$ as the population ages. The effect of the
very slowly raising f is that initially the cost influence is very light in order to promote wider space exploration,
and it only increases at later stages in order to minimize complexity. The third criteria is complexity, which is given
by the formula below.
\begin{equation}
	\text{complexity} = 2 \cdot \text{\#rules} + \text{\#conditions}
\end{equation}

\subsection{Operators}
Accordingly to the three syntactic levels of the rule-based framework (conditions, rules, rule sets), we divide the
operators into three corresponding groups. In addition, each operator is classified as having either generalizing,
specializing, or unspecified -- or independent -- behavior.

\subsubsection{Rule Set Level} This is the level of sets of $VL_1$ complexes. The operators act on whole rule sets.

\begin{itemize}
	\item \textbf{Independent}
	\begin{itemize}
		\item \textbf{Rules Exchange:} This operator requires two parent rule sets, and it exchanges random rules
		between these two. It requires two parameters: probability of application to a rule set, and probability of
		rules selection for the exchange.
	\end{itemize}
	\item \textbf{Generalization}
	\begin{itemize}
		\item \textbf{Rules Copy:} This operator requires two parent rule sets, and it copies a random rule from each
		of the sets to the other. It differs from the ``rules exchange'' operator, since it does not remove information
		being propagated from the rule set. It requires one parameter: probability of application to a rule set.
		\item \textbf{New Event:} This operator acts on a single rule set: if there is a positive event not covered yet
		by the current rule set, this event's description is added to the set as a new rule. Its actions have very
		little computational overhead.
		\item \textbf{Rules Generalization:} This operator selects two random rules and replaces them by their most
specific generalization.
	\end{itemize}
	\item \textbf{Specialization}
	\begin{itemize}
		\item \textbf{Rules Drop:} This operator drops a random rule from that set.
		\item \textbf{Rules Specialization:} This operator replaces two random rules
		by their most general specialization.
	\end{itemize}
\end{itemize}

\subsubsection{Rule Level} This the level of $VL_1$ complexes. Operators act on one rule at a time.

\begin{itemize}
	\item \textbf{Independent}
	\begin{itemize}
		\item \textbf{Rule Split:} This operator acts on a single rule, and it splits it into a number of rules,
		according to a partition of the values of a condition. The set of domain values present in the selected
		condition can be split according to each value individually or according to two disjoint subsets of values.
	\end{itemize}
	\item \textbf{Generalization}
	\begin{itemize}
		\item \textbf{Condition Drop:} This operator acts on a single rule, and it removes a present condition from
		that rule.
		\item \textbf{Conjunction $\rightarrow$ Disjunction:} This operator acts on a single rule, and it splits the
		complex into a disjunction where the complex' separation into n and m selectors is random and position
		independent.
	\end{itemize}
	\item \textbf{Specialization}
	\begin{itemize}
		\item \textbf{Condition Introduction:} This operator introduces a random condition associated with an
		unconditioned attribute. The new selector is a random choice from among all of its possible internal
		disjunctions.
		\item \textbf{Rule Directed Split:}  This operator acts on a single rule. If this rule covers a negative event,
		it is split into a set of maximally general rules that are yet consistent with that event.
	\end{itemize}
\end{itemize}

\subsubsection{Condition Level} This the level of $VL_1$ selector. Operators act on one condition at a time.

\begin{itemize}
	\item \textbf{Independent}
	\begin{itemize}
		\item \textbf{Reference Change:} This operator randomly removes or adds a single domain value to this
		condition.
	\end{itemize}
	\item \textbf{Generalization}
		\begin{itemize}
		\item \textbf{Reference Extension:} This extends the domain by allowing a number of additional values.
	\end{itemize}
	\item \textbf{Specialization}
	\begin{itemize}
		\item \textbf{Reference Restriction:} This operator removes some domain values from this condition.
	\end{itemize}
\end{itemize}

\subsubsection{Operator Selection Probability}
Each operator is given some initial probabilities from two separate groups: selection and application probabilities.
The selection probabilities serve to select one of a number of possible actions and are static. The application
probabilities serve to fire operators for structures of their type and have a dynamic character.

\begin{itemize}
	\item Probabilities of generalizing operators are increased for applications to structures (rule sets, rules,
	conditions) that are incomplete, and are decreased for those that are inconsistent.
	\item Probabilities of specializing operators are increased for applications to structures that are inconsistent,
	and decreased for those that are incomplete.
\end{itemize}

These two measures serve as an additional heuristic to the fitness. The new value $p'$ is the adjusted probability,
and $p$ is the actual probability. The value of $p'$ is computed differently for each rule and rule set.

\begin{equation}
	\text{Generalizing operators:} p' = p \cdot \left( \frac{3}{2} - \text{completeness} \right) \cdot \left(
	\frac{1}{2} + \text{consistency} \right)
\end{equation}

\begin{equation}
	\text{Specializing operators:} p' = p \cdot \left( \frac{1}{2} + \text{completeness} \right) \cdot \left(
	\frac{3}{2} - \text{consistency} \right)
\end{equation}

\section{Implementation Issues}

A prototype was implemented in C, named GIL (for Genetic-Based Inductive Learning).

\subsection{Sampling Mechanism}

GIL uses the stochastic universal sampling mechanism. Here, a roulette wheel is constructed for chromosomes with a
portion allocated proportionally to its fitness. Another wheel is constructed with equally spaced marks. The wheels are
spun against each other. A chromosome is selected once for each mark landing in its allocated space.

\subsection{Internal Representation}

Based on GABIL, GIL uses  binary digits to represent domain values and removes both invalid and empty rules. A complex
is represented by a vector of conditions. Furthermore, for simplicity and efficiency, we associate a fixed positional
correspondence between the attributes of the vector. A problem arises with representing unrestricted selectors, but
they can be dropped from its complex without any semantic change to the rule associated with this complex.

\subsection{Data Compilation}
A commonly cited disadvantage of GA is their time complexity. It is especially visible when the evaluation requires
extensive computation. Therefore, a special method of data compilation was designed. Rather than storing data in terms
of features, store features in terms of data coverage. For each possible feature, retain information about the events
covered by this feature. This must be done separately for each concept, and even for concepts not being learned. We
achieve this by enumerating all learning events and constructing binary coverage vectors. In this vector, a binary one
at position n indicates that the structure that owns this coverage vector covers event \#n. Prior to learning, as
mentioned, all data are pre-compiled into such vectors, for all possible features.

% \section{Experiment Studies}

% \subsection{Experimental Methodology}

% \subsection{Emerald's Robot World}

% \subsection{DNS Concepts}
% Learning DNF descriptions has become a standard way of evaluating different systems. GIL is compared with GABIL and
% ID5R, a decision-tree-based algorithm for supervised concept learning. The latter can be updated incrementally while
% GABIL needs to start from scratch.
% \\\\
% The original claim of the GABIL system was that it could not learn as well as ID5R on simple concepts, but achieved
% about the same levels of performance as the concepts' complexity increased. Our results show that GIL can do both at
% the same time: it achieves very high performance for all kinds of problems. It also clearly outperforms GABIL in
% terms
% of learning variability. Its smooth learning curve indicates low variability in performance, while the broken curve
% of
% GABIL indicates bigger differences from run to run. Moreover, GIL produced descriptions of much lower complexity.


% \subsection{Multiplexers}
% Each multiplexer is actually
% a specific case of the more general DNF.

% \subsection{Breast Cancer}

% \subsection{Incremental Learning}

\section{Conclusion}
This work introduces a novel approach to full-memory, supervised inductive learning in attribute-based spaces that uses
a knowledge-intensive genetic algorithm. Such an algorithm follows the ideas of traditional GAs, but replaces the
domain-independent search by a search specific to the inductive learning methodology. This approach represents an
abstraction of the traditional genetic algorithm to the symbolic level. Initial results show that GAs can be
successfully applied to more complex, non-numerical tasks by defining the algorithm at the conceptual level of the
problem. This allows for processing high-level structures using the problem-specific methodology and rich heuristics.
Moreover, such an abstract view provides for the same clear separation of different systems' components as found in AI
production systems. This modularity, in turn, allows for transparent applications of similar designs to other domains.
Some of the most interesting properties of this approach are the clearly understood processing mechanisms, defined on
the same language as that of input/output, and the low complexity of the generated knowledge. This complexity, measured
here by number of rules and conditions, contributes to the higher comprehensibility of the output.
\\\\
The GA approach is not intended to compete with existing symbolic systems in terms of time complexity. However, GA's
are naturally suited for parallelization. Additionally, the current complexity, as well as the overall quality, seems
to be unstable and to vary among different problems, different parameters used, and even different runs of the same
experiment.

\section{TLDR;}
Though GAs traditionally operate in a problem-independent way, for complex problems there can be considerable benefits to defining and using problem-specific operators. 

\end{document}