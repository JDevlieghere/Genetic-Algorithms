\documentclass[../main.tex]{subfiles}

\begin{document}

\begin{abstract}
The authors consider the application of a genetic algorithm (GA) to a symbolic learning task namely, supervised concept
learning from examples. A GA concept learner, GABL, that learns a concept from a set of positive and negative examples
is implemented. GABL is run in a batch-incremental mode to facilitate comparison with an incremental concept learner,
ID5R. Preliminary results show that, despite minimal system bias, GABL is an effective concept learner and is quite
competitive with ID5R as the target concept increases in complexity.
\end{abstract}

\begin{mdframed}
\fullcite{130359}
\end{mdframed}

\section{Introduction}

There is a misconception that GAs are primarily useful for non-symbolic learning. This is the historical result of
their heavy use for complex parameter optimizations.

\section{Supervised Concept Learning Problems}
Supervised concept learning involves inducing concept descriptions from a set of examples of a target concept. Concepts
are represented as subsets of points in an n-dimensional feature space which is defined a priori and for which all the
legal values of the features are known.

\begin{itemize}
	\item The concept description language must be sufficiently expressive to describe arbitrarily
	complex subsets of a feature space. The two language forms generally used are \textbf{decision trees} and
	\textbf{rules}.
	\item Possibly, there are infinitely many descriptions consistent with a particular set of example. This is
	resolved with \textbf{bias} toward certain kinds of representations.
	\item Evaluation and comparison of concept learners is ruled by two approaches: \textbf{batch mode} and
	\textbf{incremental mode}. The latter requires a concept description from the examples seen so far, while the
	former only requires a description when all have been seen.
\end{itemize}

\section{Genetic Algorithms and Concept Learning}
In order to apply GAs to a particular problem, we need to select an internal representation of the space to be searched
and define an external evaluation function which assigns utility to candidate solutions.

\subsection{Representing the Search Space}

Concept description are generally symbolic in nature, have both syntactic and semantic constraints and vary in length.
A traditional fixed-length representation, although theoretically well supported, seems not well suited. Two solutions
exist: changing the GA operators or construct a string representation minimizing any changes to the GA operators
without rendering them useless. The latter approach was chosen.

\subsection{Defining Fixed-length Classifier Rules}

Having chosen for a representation that minimizes the standard operators, the concept description language needs to be
chosen carefully. A disjunctive set of classification rules (DNF) is a natural way to do so.

\begin{mdframed}
\texttt{IF} \{conjunction of test involving feature values\} \\
\texttt{THEN} \{rule indicating concept\}
\end{mdframed}

Since the left-hand sides are conjunctive forms with internal disjunction, there is no loss of generality by requiring
that there be at most one test for each feature (on the left hand side of a rule).
\\\\
Each fixed-length rule will have N feature tests, one for each feature. Nominal features with $k$ values use $k$ bits,
one for each value. The right-hand side of a rule is simply the class (concept) to which the example belongs. This
means that our ``classifier system'' is a ``stimulus-response'' system with no internal memory.


\subsection{Evolving Sets of Classifier Rules}
Since a concept description will consist of one or more classifier rules, we still need to specify how GAs will be used
to evolve sets of rules. There are two basic strategies: the Michigan approach and the Pittsburgh approach. The latter
is chosen so that , each individual in the population is a variable length string representing an unordered set of
fixed-length rules.
\\\\
Crossover can occur anywhere (i.e., both on rule boundaries and within rules). The only requirement is that the
corresponding cross-over points on the two parents ``match up semantically''. That is, if one parent is being cut on a
rule boundary, then the other parent must be also cut on a rule boundary. The mutation operator is unaffected and
performs the usual bit-level mutations.

\subsection{Choosing a Payoff Function}
The payoff function is the natural place to centralize and make explicit any biases (preferences) for certain kinds of
concept descriptions. A payoff function involving only classification performance is chosen. The payoff (fitness) of
each individual rule set is computed by testing the rule set on the current set of examples and letting:

\begin{equation}
	\text{payoff}(\text{individual}_i) = (\text{percent correct})^2
\end{equation}

This provides a non-linear bias toward correctly classifying all the examples while providing differential reward for
imperfect rule sets.

\subsection{The GA Concept Learner}

The simplest approach involves using a batch mode in which a fixed set of examples is presented, and the GA must search
the space of variable-length strings described above for a set of rules which achieves a score of 100\%. We will call
this approach GABL (GA Batch concept Learner). The search terminates as soon as a 100\% correct rule set is found
within a user-specified upper bound on the number of generations.
\\\\
The simplest way to produce an incremental GA concept learner is to use GABL incrementally in the following way. The
concept learner initially accepts a single example from a pool of examples. GABL is used to create a 100\% correct rule
set for this example. This rule set is used to predict the classification of the next example. If the prediction is
incorrect, GABL is invoked to evolve a new rule set using the two examples. If the prediction is correct, the example
is simply stored with the previous example and the rule set remains unchanged. As each new additional instance is
accepted, a prediction is made, and the GA is re-run in batch if the prediction is incorrect. We refer to this mode of
operation as batch-incremental and we refer to the GA batch-incremental concept learner as GABIL.

\section{Empirical Studies}

\subsection{Evaluating Concept Learning Programs}

An incremental concept learner will make a prediction for each new instance seen. Each prediction is either correct or
incorrect. We are interested in examining how an incremental system changes its predictive performance over time. We
examine a small window of recent outcomes, counting the correct predictions within that window. Performance curves can
then be generated which indicate whether a concept learner is getting any better at correctly classifying new (unseen)
examples

\subsection{Initial Experiments}

\begin{itemize}
	\item Demonstrate the predictive performance of GABIL as a function of incremental increases in the size and
	complexity of the target concept.
	\item Compare performance with an existing algorithm, Utgoff’s ID5R, which is a well-known incremental concept
	learning algorithm.
\end{itemize}

\subsubsection{ID5R vs GABIL}

\begin{itemize}
	\item ID5R relies upon Quinlan’s information theoretic entropy measure to build its decision trees. This measure
	works well when individual features are meaningful in distinguishing an example as positive or negative.  ID5R’s
	information theoretic biases will therefore perform better on simpler target concepts.
	\item GABIL, however, should perform uniformly well on target concepts of varying complexity.There is also no bias
	towards a small number of disjuncts.
\end{itemize}

Given these biases (and lack of biases), it is natural to expect that while ID5R will outperform GABIL on the simpler
concepts, but there will exist a frontier at which the situation will reverse.

\subsubsection{Results}

ID5R’s biases favor concepts that can be represented with small decision trees GABIL does not have these biases, and
appears to be less sensitive to increasing numbers of disjuncts and conjuncts. GABIL does not degrade significantly
with increasing target concept complexity and outperforms ID5R on 4 disjunct concepts. Since the syntactic complexity
of a target concept corresponds roughly with the size of its decision tree representation, we expect this trend to
continue with more difficult target concepts.

\section{Conclusion}

This paper presents a series of initial results regarding the use of GAs for symbolic learning tasks. In particular, a
GA-based concept learner is developed and analyzed. It is interesting to note that reasonable performance is achieved
with minimal bias. There is no preference for shorter rule sets, unlike most other concept learning systems. The
initial results support the view that GAs can be used as an effective concept learner although they may not outperform
algorithms specifically designed for concept learning when simple concepts are involved. This paper also sets the stage
for additional comparisons between GAs and other concept learning algorithms. We feel that such comparisons are
important and encourage the research community to develop additional results on these and other problems of interest.

\end{document}